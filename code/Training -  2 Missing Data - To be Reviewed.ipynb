{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in the last notebook, we imported the data and then took a high level look at the data. For this notebook, we are going to jump into the data, exploring it a little bit more and then cleaning it by handling incorrect data and missing data, both of which will be explained in detail later. The ultimate goal of this notebook is to get our data to a place where it is ready to be used to analyzed and used to derive some insight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest Data\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\jpau\\Documents\\Projects\\Airforce\\data\\flight_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when working with data, often the datasets that you will receive will be not fit for use right away. This may be for a variety of reasons such a missing data or data that is in an unusable format or even data is that incorrect. So, generally, the first step in understanding the data is to explore and handle these unclean and missing portions of data. \n",
    "\n",
    "To do this we are going to see what data is missing. To do this we are going to use the isnull() method and the sum() method. In effect, the isnull() method tells you whether or not an element is null (doesn't contain any information) and the sum() method will add up all of these nulls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airport                     0\n",
       "track                       0\n",
       "stid                        0\n",
       "call_sign                   0\n",
       "time                        0\n",
       "event                  193709\n",
       "status                      0\n",
       "departure_airport      329940\n",
       "destination_airport    329940\n",
       "timestamp                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing Info\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there seems to be 3 columns with nulls in them, event, departure_airport, and destination_airport. Generally, there are 2 ways in which you can go about working with missing data, either you can impute or guessimate them based off prior knowledge or some statistical technique or you can remove them. \n",
    "\n",
    "For the departure and destination airports, in general incomplete flights are not all that useful, so we will remove those. In general, this data is derived form the SPDPS flight plan data, therefore there might be a problem that we cannot control. So we will drop observations with these missing values. \n",
    "\n",
    "Before we drop these null values, let us actually take a look at these observations. To do this will we need to subset the data with only those that have a null value in the departure airport and destination airport. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airport</th>\n",
       "      <th>track</th>\n",
       "      <th>stid</th>\n",
       "      <th>call_sign</th>\n",
       "      <th>time</th>\n",
       "      <th>event</th>\n",
       "      <th>status</th>\n",
       "      <th>departure_airport</th>\n",
       "      <th>destination_airport</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KPIT</td>\n",
       "      <td>3534</td>\n",
       "      <td>339554</td>\n",
       "      <td>SWA4052</td>\n",
       "      <td>2020-01-01 00:00:08</td>\n",
       "      <td>off</td>\n",
       "      <td>airborne</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KMSY</td>\n",
       "      <td>4057</td>\n",
       "      <td>68290</td>\n",
       "      <td>SWA5830</td>\n",
       "      <td>2020-01-01 00:00:08</td>\n",
       "      <td>on</td>\n",
       "      <td>onsurface</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>KSFO</td>\n",
       "      <td>3636</td>\n",
       "      <td>3605541</td>\n",
       "      <td>UAL595</td>\n",
       "      <td>2020-01-01 00:00:43</td>\n",
       "      <td>on</td>\n",
       "      <td>onsurface</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>KCLE</td>\n",
       "      <td>3430</td>\n",
       "      <td>3486145</td>\n",
       "      <td>UAL1530</td>\n",
       "      <td>2020-01-01 00:02:01</td>\n",
       "      <td>off</td>\n",
       "      <td>airborne</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>KMCI</td>\n",
       "      <td>2491</td>\n",
       "      <td>3857372</td>\n",
       "      <td>SWA1376</td>\n",
       "      <td>2020-01-01 00:02:47</td>\n",
       "      <td>off</td>\n",
       "      <td>airborne</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   airport  track     stid call_sign                 time event     status  \\\n",
       "1     KPIT   3534   339554   SWA4052  2020-01-01 00:00:08   off   airborne   \n",
       "2     KMSY   4057    68290   SWA5830  2020-01-01 00:00:08    on  onsurface   \n",
       "13    KSFO   3636  3605541    UAL595  2020-01-01 00:00:43    on  onsurface   \n",
       "30    KCLE   3430  3486145   UAL1530  2020-01-01 00:02:01   off   airborne   \n",
       "47    KMCI   2491  3857372   SWA1376  2020-01-01 00:02:47   off   airborne   \n",
       "\n",
       "   departure_airport destination_airport            timestamp  \n",
       "1                NaN                 NaN  2020-01-01 00:00:00  \n",
       "2                NaN                 NaN  2020-01-01 00:00:00  \n",
       "13               NaN                 NaN  2020-01-01 00:00:00  \n",
       "30               NaN                 NaN  2020-01-01 00:00:00  \n",
       "47               NaN                 NaN  2020-01-01 00:00:00  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = df['departure_airport'].isnull()\n",
    "\n",
    "df[index].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that indeed there is nothing we can really do to fix these airports so let's drop them. To do this, we first need to subset the data to grab all those that are not null, and then we can make a new dataset with them. In this case we will use a method that is very similar to the isnull() method, it is called the notnull() method. It does exactly the opposite of the isnull() method. Next, we will subset the data in order and assign it to a new dataframe. Why don't you try doing this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise \n",
    "\n",
    "index = None\n",
    "new_df = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "index = df['departure_airport'].notnull()\n",
    "new_df = df[index].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Let us do a check to see if the null values in the departure airport column were actually removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns and their types:\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "# Exercise\n",
    "\n",
    "answer = None\n",
    "print(\"Columns and their types:\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns and their types:\n",
      " airport                    0\n",
      "call_sign                  0\n",
      "time                       0\n",
      "event                  26717\n",
      "departure_airport          0\n",
      "destination_airport        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Answer\n",
    "\n",
    "answer = new_df.isnull().sum()\n",
    "print(\"Columns and their types:\\n\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to notice is that when we took care of all the missing departure airports, we also took care of all the destination airports. Nice! So let's now move onto the missing events. \n",
    "\n",
    "First thing with the events column is that we are actually going to take a look at what kinds of events there are. We can do this by subsetting the data and then applying the unique() method to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['on', 'off', nan], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['event'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that there are only 2 events, Off and On. These represent take offs and landings respectively.\n",
    "\n",
    "Given that there are only two unique values and based off the reality that we can actually fill these in based upon the airport column and departure/destination, instead of dropping these columns, let's actually try to impute them. So to clarify, if the airport column matches the departure column, then we know the event must be a take off and if it matches the destination column, then the event must be a landing. Thus, we can assign values based on these things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_index = (new_df['event'].isnull()) & (new_df['airport'] == new_df['departure_airport'])\n",
    "new_df.loc[on_index,'event'] = 'Off'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we provide the two conditions for the missing events that we are looking for. They must first be null and secondly, they must be a take off. If these two conditions are met, then we can assign them to a value of 'Off'. You'll notice that there is a &. This is an bitwise operator that indicates that both conditions need to be true in order to return a true value. The usage of .loc in subsetting is just another way to subset the data. It is primarily used for label based indexing (eg. usage of the column 'events). Why don't you try to impute the values of the takeoffs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise\n",
    "\n",
    "off_index = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "off_index = (new_df['event'].isnull()) & (new_df['airport'] == new_df['destination_airport'])\n",
    "new_df.loc[off_index,'event'] = 'On'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that that is done. Let us check to see if we have any last missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airport                  0\n",
       "track                    0\n",
       "stid                     0\n",
       "call_sign                0\n",
       "time                     0\n",
       "event                  118\n",
       "status                   0\n",
       "departure_airport        0\n",
       "destination_airport      0\n",
       "timestamp                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, there seems to be a few more missing values. Let's take a look at these rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise\n",
    "\n",
    "index = None\n",
    "new_df[index].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airport</th>\n",
       "      <th>call_sign</th>\n",
       "      <th>time</th>\n",
       "      <th>event</th>\n",
       "      <th>departure_airport</th>\n",
       "      <th>destination_airport</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>157970</th>\n",
       "      <td>KMDW</td>\n",
       "      <td>SWA1975</td>\n",
       "      <td>2020-07-30 17:53:35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KMCO</td>\n",
       "      <td>KPHL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221334</th>\n",
       "      <td>KSNA</td>\n",
       "      <td>SWA1908</td>\n",
       "      <td>2020-03-11 01:41:16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KPHX</td>\n",
       "      <td>KSEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313461</th>\n",
       "      <td>KBWI</td>\n",
       "      <td>SWA28</td>\n",
       "      <td>2020-01-28 02:02:10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KHOU</td>\n",
       "      <td>KDAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331997</th>\n",
       "      <td>KLAX</td>\n",
       "      <td>AAL2597</td>\n",
       "      <td>2020-02-10 05:28:09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KDFW</td>\n",
       "      <td>KBUR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338196</th>\n",
       "      <td>KSTL</td>\n",
       "      <td>AAL2408</td>\n",
       "      <td>2020-07-19 17:54:10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KCLT</td>\n",
       "      <td>KORD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       airport call_sign                 time event departure_airport  \\\n",
       "157970    KMDW   SWA1975  2020-07-30 17:53:35   NaN              KMCO   \n",
       "221334    KSNA   SWA1908  2020-03-11 01:41:16   NaN              KPHX   \n",
       "313461    KBWI     SWA28  2020-01-28 02:02:10   NaN              KHOU   \n",
       "331997    KLAX   AAL2597  2020-02-10 05:28:09   NaN              KDFW   \n",
       "338196    KSTL   AAL2408  2020-07-19 17:54:10   NaN              KCLT   \n",
       "\n",
       "       destination_airport  \n",
       "157970                KPHL  \n",
       "221334                KSEA  \n",
       "313461                KDAL  \n",
       "331997                KBUR  \n",
       "338196                KORD  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer\n",
    "\n",
    "index = new_df['event'].isnull() \n",
    "new_df[index].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like neither the departure airport nor the destination airport matches the airport column. This is weird and because we don't know whether it is a take off or landing. Let us remove these observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise\n",
    "index = None\n",
    "complete_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "index = new_df['event'].notnull()\n",
    "complete_df = new_df[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do one last check to see if we are finally done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airport                0\n",
       "track                  0\n",
       "stid                   0\n",
       "call_sign              0\n",
       "time                   0\n",
       "event                  0\n",
       "status                 0\n",
       "departure_airport      0\n",
       "destination_airport    0\n",
       "timestamp              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hurray! We now have a complete dataset to work with. Before we can move on to analyzing it, we need to see if all of the data in the dataset is correct. We will do this in the next section"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Used only for building Scripts\n",
    "complete_df.to_csv(r'C:\\Users\\jpau\\Documents\\Projects\\Airforce\\data\\complete_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
